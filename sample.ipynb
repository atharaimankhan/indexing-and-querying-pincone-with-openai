{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Libraries\n",
    "from pinecone import Pinecone\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading enviroment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets read the document\n",
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    document = file_loader.load()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = read_doc('documents/')\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the docs into chunks using splitter\n",
    "\n",
    "def chunk_data(docs, chunk_size=800, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='8 Years Experience\\nShan Ali Khan@ ADDO AI\\nSenior Data Scientist\\nContact\\nPhone\\n+92 346 467 4070\\nAddress\\nWapda Town, Lahore - PK\\nEmail\\nshanalikhan@hotmail.com\\nExpertise\\nSkill setData Analysis & Visualization\\nMLOpsNatural Language Processing\\nExperiencesEducational\\n2016 - 2018 Masters in Computer Science\\nMachine Learning, Natural Language Processing, Statistical Modeling, ...\\nPunjab University College Of Information Technology\\nLahore, Pakistan \\nSystem Analysis & Design\\nAzure\\nSynapse\\nData Bricks, ML Studio\\nML\\nLanguage Modeling\\nNER\\nSpeech AnalysisDocument Classification\\nTime-series Analysis2010 - 2014 Bachelors in Computer Science\\nObject Orient Programming, Data Structures, Software Engineering\\nUniversity of Central Punjab\\nLahore, Pakistan\\nSenior Data Scientist - ADDO AI', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 0}),\n",
       " Document(page_content='Lahore, Pakistan\\nSenior Data Scientist - ADDO AI\\nApril 2021 - PresentTools / Techniques\\nAutomatic Speech Recognition\\nMain responsibilities are:\\nDomain & problem understanding\\nLead Team of Data Scientists and Python Eng.\\nExploratory Data Analysis and research\\nSystem Design & Release Planning\\nPerformed Voice activity detection on call center calls.\\nFine-tuned Wav2Vec2 model for singlish language to generate\\ntranscript for calls.\\nDesigned machine learning approach to extract user\\nsentiments, behavior and rules to evaluate the agents of call\\ncenter.\\nUsed metric learning for further analyzing the different criteria\\nto evaluate the agents.Pytorch\\nMetric Learning\\nUniversal Sentence Embedding\\nKenLM\\nWav2Vec2\\nCI / CD & DockersPredictive Modeling & Forecasting\\nAWS\\nSagemaker , EC2Spark , RabbitMQ', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 0}),\n",
       " Document(page_content='AWS\\nSagemaker , EC2Spark , RabbitMQ\\nQuestion Answering SystemExplainable AIOpen Source & Hackathons\\n40+ Contributors\\n4000 GitHub stars and ~400 forks.\\nFeatured in Microsoft magazine (MSDN)\\n1 Million usersVSCode Settings Sync - VSCode Extension\\nOpenAI Stack Hackathon\\nBuilt POC - 360 view from Open AI Dall-E 2 model and implemented out\\npainting using OpenAI models within few days for interior design rooms\\nBuild Your Startup Hackathon\\nBuilt POC - Used whisper to implement speech to text as a call center\\nand automated actions based on user speech\\nGCP\\nVertex AI', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 0}),\n",
       " Document(page_content='Tools / Techniques\\nPrincipal Machine Learning Engineer - Accelirate\\nAugust 2018 - March 2021\\nIntelligent SchedularMain responsibilities are:\\nLead Team of Data Scientists and Python Eng.\\nSystem Analysis & Design\\nRelease Planning\\nPerformed Exploratory Data Analysis to evaluate how the utilization rate of VMs\\ncan be improved.\\nBuilt automated Time Series Analysis to forecast the next upcoming job time.\\nIntegrated RabbitMQ queues to schedule the jobs across multiple machines. \\nDeveloped and maintained Kibana dashboards for visualization\\nBuilt using Azure ML Pipeline, Synapse, Blob Storage, Data Lake\\n \\nOpenBots Documents\\nTrained models for Document Classification\\nApplied Information Retrieval techniques (NER, etc) to extract relevant entities &\\nfields from document', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 1}),\n",
       " Document(page_content='fields from document\\nAutomated model maintenance using Azure ML pipelines to evaluate new data\\nand deploy new model after evaluating based on new data \\nFine-tuned Tesseract OCR for some documents and used Google OCR.\\nPrincipal Software Engineer\\nApril 2017 - August 2018\\nMain responsibilities are:\\nIntegrate different retail processes with Dynamics 365 & Provide functional\\nsupport\\nSystem Analysis & Design\\nLead product team & TrainingTime-series Analysis\\nNamed-Entity RecognitionTesseract fine-tuning\\nRabbitMQDocument Classification\\nMicroservices\\nElasticsearch & Kibana \\nAzure Web App\\nMicrosoft Retail SDKMicrosoft Dynamics 365Extractive Question Answering System\\nDeveloping question answering system for IT and HR support\\nso employees can get answers instead reaching out to', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 1}),\n",
       " Document(page_content='support. Main responsibilities are:\\nEDA & Domain Understanding\\nLead Team of Data Scientists and Python Eng.\\nParticipated in from model building till deployment\\nBuilt document store using Elastic Search Node\\nBuilt extractive QA Pipeline by building embedding based\\nretrieval model to identify the most relevant document\\nBuild extractive LongFormer model to extract the answer from\\nthe document \\nDeployed and served as Azure ML APIHayStack\\nQuestion Answering\\nLongFormer Model\\nEmbedding Based retrievalDisease Profiling & Length Of Stay Prediction\\nMain responsibilities are:\\nDomain & problem understanding\\nLead Team of Data Scientists and Python Eng.\\nExploratory Data Analysis and research\\nSystem Design & Release Planning\\nIdentified locations and individuals where covid is more\\nvulnerable', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 1}),\n",
       " Document(page_content=\"vulnerable\\nIdentified length of stay in hospitals based on individual's\\nprofile\\nDeveloped Google Vertex AI pipelines and served vertex\\nendpoints as wellData Sampling\\nVertex AIBigQuery\\nMetric Learning\\nCI / CD & Dockers\", metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 1}),\n",
       " Document(page_content='Tools / Techniques\\nMigration from SQL to No-SQL\\nDeveloped complete end to end database migration from SAP HCM Oracle\\ndatabase to elastic search to improve the caching the fast retrieval. \\nOpenXML Support for Powerpoint and PDF\\nIntegrated OpenXML support to convert the hierarchical entities to generate\\nMicrosoft Power Point visualizations and integrated PDF conversion.\\nAccessibility Tools\\nDeveloped configurable dynamic speech features that covered all the web\\nelements in the application and allows speech phrases to be completed\\nconfigured by admin side. Dynamic Tab Indexing integrated in the\\napplication including menus and in multiple dialogs using Depth scan.  Features Developed:\\nSoftware Engineer\\nSeptember 2014 - February 2015\\nMain responsibilities are:\\nApplication Development', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 2}),\n",
       " Document(page_content='Application Development\\nSystem Enhancement & Support\\nNopCommerce POS Integration\\nProject completed successfully by integrating of point-of-sale system with\\nNopCommerce. Plugin developed in NopCommerce providing Restful API\\nconnected with the client web application.\\nCustom CRM Support\\nCustom CRM support was provided and maintained all the development\\nand support work.Accessibility Configurations\\nOpenXML\\nASP.NET MVC\\nAngularJS\\nNopCommerce\\nSalesForce CRMOracle\\nPublications & Open Source\\nLanguage Modeling using deep learning in roman Urdu\\nSentiment Analysis using deep learning in roman Urdu\\nBuilding machine transliteration system using deep learning for Arabic\\nlanguageCompleted several researches that includes:\\nSenior Software Engineer\\nMarch 2015 - April 2017\\nMain responsibilities are:', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 2}),\n",
       " Document(page_content='Main responsibilities are:\\nApplication Development\\nSystem Enhancement\\nTraining & SupportSAP HANA\\nSpring BootAngularJSElastic SearchApplication Dashboards - Single Page Application\\nDeveloped dashboards to allow customers to visualize the jobs status and\\nentities information flowing across eCommerce and Dynamics 365\\nCloud Deployment and Continuous Delivery & Integration\\nImplemented process of CD and CI using Azure\\nEnd to end complete cloud migration to Azure App Services and Azure API\\nmanagementMiddle ware to connect any eCommerce solution with Microsoft Dynamics 365.\\nAllows different jobs to schedule to pass sales order and sync other retails entities\\nwith eCom. and D365 \\n.Commerce Link ASP.NET MVC\\nAngular\\nMongoDB', metadata={'source': 'documents/Shan+Ali+Khan.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_data(docs=doc)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7b5d1032aef0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7b5ce1e612a0>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## Embedding Technique of OpenAI\n",
    "embeddings = OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY'])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectors\n",
    "vectors = embeddings.embed_query(\"I just want to ask you that how are you doing?\")\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7b5ce18399f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to Pincone Index and Inserting chunked docs:\n",
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "index_name = os.environ['PINECONE_INDEX_NAME']\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b5ce0b07160>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b5ce20e1bd0>, temperature=0.5, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x7b5ce18399f0>, search_kwargs={'k': 2}))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using a Chain\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0.5),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 2}),\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'whats the name of the candidate and how much experience does it have overall?', 'result': 'The name of the candidate is Shan Ali Khan, and they have 8 years of overall experience.'}\n"
     ]
    }
   ],
   "source": [
    "our_query = \"whats the name of the candidate and how much experience does it have overall?\"\n",
    "answer = chain.invoke(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"Tell some of the candidate's skill/technologies which is common in his multiple experiences?\", 'result': \"Based on the provided context, some of the common skills/technologies used in the candidate's multiple experiences include:\\n\\n1. Database Migration: The candidate has experience in migrating databases, such as from SAP HCM Oracle to Elastic Search, indicating proficiency in database migration techniques.\\n\\n2. Integration of Tools: The candidate has integrated various tools like OpenXML for Powerpoint and PDF conversion, indicating expertise in tool integration.\\n\\n3. Machine Learning: The candidate has experience as a Principal Machine Learning Engineer, showcasing skills in machine learning techniques, data analysis, and model training.\\n\\n4. Data Visualization: The candidate has developed and maintained Kibana dashboards for visualization, demonstrating proficiency in data visualization tools.\\n\\n5. Automation: The candidate has built automated Time Series Analysis and integrated RabbitMQ queues for job scheduling, showing expertise in automation techniques.\\n\\n6. Software Development: The candidate has experience in application development and system analysis & design, indicating strong software engineering skills.\\n\\nThese are some of the skills and technologies that are common across the candidate's multiple experiences.\"}\n"
     ]
    }
   ],
   "source": [
    "our_query = \"Tell some of the candidate's skill/technologies which is common in his multiple experiences?\"\n",
    "answer = chain.invoke(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"Is candidate's work experience is relevant to his education / formal study? If there is any irrelevant experience or task please list it.\", 'result': \"Based on the provided work experience, it seems that the candidate's work aligns well with their education/formal study. The candidate has experience in EDA, domain understanding, leading teams of data scientists and Python engineers, model building, deployment, database migration, and developing various tools and techniques that are relevant to their field of study.\\n\\nThere is no clear indication of any irrelevant experience or tasks listed in the provided context.\"}\n"
     ]
    }
   ],
   "source": [
    "our_query = \"Is candidate's work experience is relevant to his education / formal study? If there is any irrelevant experience or task please list it.\"\n",
    "answer = chain.invoke(our_query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
